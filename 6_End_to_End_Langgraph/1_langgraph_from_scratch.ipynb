{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def function_1(input_1):\n",
    "    return input_1 + \" First Function \"\n",
    "\n",
    "def function_2(input_2):\n",
    "    return input_2 + \"to Second Function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "workflow.add_edge('node_1', 'node_2')\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am moving from First Function to Second Function'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "app.invoke('I am moving from')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from the node 'node_1':\n",
      "----\n",
      "I am moving from First Function \n",
      "\n",
      "--\n",
      "\n",
      "output from the node 'node_2':\n",
      "----\n",
      "I am moving from First Function to Second Function\n",
      "\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"I am moving from\"\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"output from the node '{key}':\")\n",
    "        print(\"----\")\n",
    "        print(value)\n",
    "    print(\"\\n--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating LLM call in the LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 30, 'total_tokens': 40, 'completion_time': 0.05, 'prompt_time': 0.003675701, 'queue_time': 0.054126363, 'total_time': 0.053675701}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-7fdd743b-5f63-4309-85d7-4eef85d9f39d-0', usage_metadata={'input_tokens': 30, 'output_tokens': 10, 'total_tokens': 40})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model_name=\"qwen-2.5-32b\")\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the topic based on the user query. \\\n",
    "        Only output the topic among: [Japan , Sports]. Don't include reasoning. Following is the user query: \" + input_1\n",
    "    response = llm.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    TOPIC_UPPER = input_2.upper()\n",
    "    response = f\"Here is the topic in UPPER case: {TOPIC_UPPER}\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"Agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "workflow.add_edge('Agent', 'tool')\n",
    "\n",
    "workflow.set_entry_point(\"Agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the topic in UPPER case: JAPAN'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Tell me about Japan's Industrial Growth\"\n",
    "app.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'Agent':\n",
      "---\n",
      "Japan\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "Here is the topic in UPPER case: JAPAN\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app.stream(query):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the txt files from source directory\n",
    "\n",
    "\n",
    "loader = DirectoryLoader('D:\\LangGraph\\Langraph-tutorial\\LangGraph\\data', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "### Creating Chunks using RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###  BGE Embddings\n",
    "\n",
    "# from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "# model_kwargs = {'device': 'cpu'}\n",
    "# encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "# embeddings = HuggingFaceBgeEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs=model_kwargs,\n",
    "#     encode_kwargs=encode_kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D:\\\\LangGraph\\\\Langraph-tutorial\\\\LangGraph\\\\data\\\\japan.txt'}, page_content=\"Japan's last four year GDP:\"), Document(metadata={'source': 'D:\\\\LangGraph\\\\Langraph-tutorial\\\\LangGraph\\\\data\\\\japan.txt'}, page_content='Industrial revival hope for Japan'), Document(metadata={'source': 'D:\\\\LangGraph\\\\Langraph-tutorial\\\\LangGraph\\\\data\\\\japan.txt'}, page_content=\"Japanese industry is growing faster than expected, boosting hopes that the country's retreat back\"), Document(metadata={'source': 'D:\\\\LangGraph\\\\Langraph-tutorial\\\\LangGraph\\\\data\\\\japan.txt'}, page_content=\"exports, normally the engine for Japan's economy in the face of weak domestic demand, had helped\")]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Tell me about japan\"\n",
    "docs = retriever.invoke(query)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assign AgentState as an empty dict\n",
    "AgentState = {}\n",
    "\n",
    "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes. \n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': []}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[-1]   ## Fetching the user question\n",
    "    \n",
    "    complete_query = \"Your task is to provide only the topic based on the user query. \\\n",
    "        Only output the topic among: [Japan , Sports]. Don't include reasoning. Following is the user query: \" + question\n",
    "    response = llm.invoke(complete_query)\n",
    "    state['messages'].append(response.content) # appending LLM call response to the AgentState\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"Agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "workflow.add_edge('Agent', 'tool')\n",
    "\n",
    "workflow.set_entry_point(\"Agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the information provided, Japan is currently experiencing positive economic and industrial growth. There is an indication of industrial revival, with the industry growing faster than expected. This growth is contributing to the hope that Japan's economic retreat might be reversing. Additionally, despite weak domestic demand, exports have been a significant contributor to the economy, acting as a key engine for economic activity. However, specific details about Japan's last four years of GDP are mentioned but not detailed in the given context.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = {\"messages\": [\"Tell me about japan\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, here is the information about Japan's GDP for the last four years:\\n\\n- **2022:** $4,256.41B\\n- **2021:** $5,034.62B\\n- **2020:** $5,055.59B\\n- **2019:** $5,117.99B\\n\\nThese figures represent the Gross Domestic Product (GDP) of Japan for the specified years.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"tell me about gdp of japan\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'Agent':\n",
      "---\n",
      "{'messages': ['tell me about gdp of japan', 'Japan', 'Japan']}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "Based on the information provided, here is a summary of Japan's GDP over the last four years:\n",
      "\n",
      "- **2019**: $5,117.99B\n",
      "- **2020**: $5,055.59B\n",
      "- **2021**: $5,034.62B\n",
      "- **2022**: $4,256.41B\n",
      "\n",
      "The data indicates a general trend of decreasing GDP from 2019 to 2022, with 2022 showing a particularly notable drop.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplyfying the State addition and maintainence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
